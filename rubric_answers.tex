\documentclass{article}
\usepackage[margin=0.75in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{upquote}
\usepackage{parskip}

\author{Caroline}
\title{Data Wrangle OpenStreetMaps Data}

\begin{document}
    \maketitle
    \section{Problems Encountered in the Map}
        I began by running the street name auditing that I wrote for the Lesson
        6 exercise, and I found a handful of abbreviated or misspelled street
        suffixes (e.g. ``Ave'' or ``Avnue'' instead of ``Avenue''). Based on
        these, I created the file \texttt{street\_mapping.json} to map ``messy''
        street names to clean ones, and later applied this mapping.

        While looking at street suffixes, I noticed some that were not
        capitalized, which made me think of checking capitalization in general.
        I wrote a function to normalize the captalization of names, and while I
        was at it, I normalized the whitespace as well. Then I checked which
        street names would be changed by this modification, and based on the
        results I rewrote the function until normalizing street names would
        improve any street name that it changed. Once this was done, I performed
        the normalization. Under this normalization, \verb"' Albert Sabin Way'"
        was changed to \verb"'Albert Sabin Way'" and \verb"'hamilton Avenue'"
        was changed to \verb"'Hamilton Avenue'". I applied normalization before
        fixing street suffixes so that I would not have to include lowercase
        street suffixes in my mapping.

    \section{Overview of the Data}
        \subsection{Output from initial cleaning programs}
            \subsubsection{}
            The \texttt{count\_tags.py} program output was
            \begin{lstlisting}
            {'bounds': 1,
             'member': 14358,
             'nd': 2376169,
             'node': 1926749,
             'osm': 1,
             'relation': 1145,
             'tag': 750748,
             'way': 262715}
            \end{lstlisting}

            \subsubsection{}
            First I ran the \texttt{count\_key\_types.py} as written for the
            exercise, and the output was
            \begin{lstlisting}
            [('lower', 443987), ('lower_colon', 294418), ('other', 12343)]
            \end{lstlisting}
            This means that there were no ``problematic characters'', however,
            there were quite a few keys of type ``other'', which could mean many
            different things. There could be uppercase characters, non-ascii
            characters, multiple colons in a single field name, or other strange
            characters not listed in the problematic characters set.

            To investigate this, I updated \texttt{count\_key\_types.py} to
            include unicode lowercase characters within the ``lower'' and
            ``lower\_colon'' classes, and I added the classes
            ``alpha\_with\_upper'' for keys with only letters and underscores,
            but at least one uppercase, and ``word\_plus\_colon'' for keys
            containing alphanumeric characters, underscores, and colons. I also
            modified ``lower\_colon'' to accept multiple colons. After this, the
            program output was 
            \begin{lstlisting}
            [('lower', 443987),
             ('lower_colon', 295889),
             ('word_plus_colon', 10459),
             ('alpha_with_upper', 373),
             ('other', 40)]
            \end{lstlisting}
            Thus, it looks like most of the ``other'' keys were keys that
            contained some mix of letters, numbers, underscores, and colons.
            
            Since at this point there were only 40 ``other'' keys left, I
            decided to print them out, and it turned out all of them contained
            hyphens.

            \subsubsection{}
            By running \texttt{find\_users.py}, I determined that 536 unique
            users have contributed to the Cincinnati data.
        \subsection{Output from Mongo queries on cleaned data}

        \subsubsection{}
        First, I ran findOne to verify the format of the documents.

        \begin{lstlisting}
        > db.cincinnati.findOne()
        {
            "_id" : ObjectId("556b5a56b589e243caae8ea8"),
            "id" : "60513020",
            "type" : "node",
            "pos" : [
                39.4307416,
                -84.341689
            ],
            "created" : {
                "uid" : "33757",
                "changeset" : "12642091",
                "version" : "17",
                "user" : "Minh Nguyen",
                "timestamp" : "2012-08-07T08:15:12Z"
            }
        }
        \end{lstlisting}

        \subsubsection{}
        I tested my setup by finding the number of users again, but using
        a Mongo query.

        \begin{lstlisting}
        > db.cincinnati.distinct('created.uid').length
        516
        \end{lstlisting}

        This surprised me, but then I realized I had only added "node" and "way"
        tags to the Mongo database, whereas \texttt{find\_users.py} had counted
        users for all tag types.

        \subsubsection{}
        I also checked which users were the top contributors.

        \begin{lstlisting}
        > db.cincinnati.aggregate([
        ...     {'$group': {'_id': '$created.uid', 'count': {'$sum': 1}}},
        ...     {'$sort': {'count': -1}},
        ...     {'$limit': 5}
        ... ])

        { "_id" : "33757", "count" : 900034 }
        { "_id" : "1813535", "count" : 566050 }
        { "_id" : "147510", "count" : 347045 }
        { "_id" : "1108251", "count" : 233693 }
        { "_id" : "451693", "count" : 13771 }
        \end{lstlisting}

        \subsubsection{}
        Then I checked how many elements had been added to the collections.

        \begin{lstlisting}
        > db.cincinnati.count()
        2189464
        \end{lstlisting}

        This is the same as the number of "node" tags plus the number of "wag"
        tags as output by \texttt{count\_tags.py}.

        \subsubsection{}
        Then I checked for the most common street names in Cincinnati:

        \begin{lstlisting}
        > db.cincinnati.aggregate([
        ...     {'$match': {'address.street': {'$ne': null}}},
        ...     {'$group': {'_id': '$address.street', 'count': {'$sum': 1}}},
        ...     {'$sort': {'count': -1}},
        ...     {'$limit': 10}
        ... ])

        { "_id" : "Wards Corner Road", "count" : 110 }
        { "_id" : "Main Street", "count" : 102 }
        { "_id" : "Enyart Road", "count" : 89 }
        { "_id" : "Branch Hillâ€“Loveland Road", "count" : 88 }
        { "_id" : "Sunrise Drive", "count" : 82 }
        { "_id" : "Rosemont Avenue", "count" : 78 }
        { "_id" : "Heidelberg Drive", "count" : 77 }
        { "_id" : "Stockton Drive", "count" : 75 }
        { "_id" : "Glen Lake Road", "count" : 74 }
        { "_id" : "Lindenhall Drive", "count" : 69 }
        \end{lstlisting}

        \subsubsection{}
        I checked the minimum and maximum latitude and longitude:

        \begin{lstlisting}
        > db.cincinnati.aggregate([
        ...     {'$unwind': '$pos'},
        ...     {'$group': {
        ...         '_id': '$_id',
        ...         'lat': {'$first': '$pos'},
        ...         'lon': {'$last': '$pos'}}},
        ...     {'$group': {
        ...         '_id': 'All',
        ...         'maxLat': {'$max': '$lat'},
        ...         'minLat': {'$min': '$lat'},
        ...         'maxLon': {'$max': '$lon'},
        ...         'minLon': {'$min': '$lon'}}}
        ... ], {'allowDiskUse': true})

        { "_id" : "All", "maxLat" : 39.4709987, "minLat" : 38.875,
         "maxLon" : -84.1210019, "minLon" : -84.842999 }
        \end{lstlisting}

        This gives essentially the same result as that listed in the bounds tag
        in the XML file, thus validating that tag.

        \begin{lstlisting}
        <bounds minlat="38.875" minlon="-84.843"
                maxlat="39.471" maxlon="-84.121"/>
        \end{lstlisting}

        
    \section{Other Ideas about the Dataset}
    An interesting next task might be to compare datasets from multiple cities
    or areas. Is the relative number of documents in different cities
    proportional to the relative sizes of the cities? If not, one of the cities
    may have very incomplete data. Are street suffix proportions relatively
    constant across different areas, or do some areas use certain suffixes more
    heavily relative to other areas? Are the same users contributing to multiple
    different areas, or does each area have its own set of users that
    contributes to it? This information could be used to identify problems in
    one area that are not present in another or analyze cultural differences
    between place names in different areas.

    For a small number of areas, these comparisons could be performed in a
    similar method as to this project, but by loading multiple areas into
    separate collections. (This extension would work since right now I have
    loaded the entire dataset for Cincinnati into a single collection.) Then
    a Mongo query, to e.g., find the most common street suffixes, could be
    performed on each collection and the results compared. However, this process
    could get tedious for a large number of areas.

    Perhaps a more scalable approach than the above would be to continue to keep
    all the data in a single collection, but add a new field to indicate what
    area the data is from. Then Mongo aggregation queries could be used to group
    by the area name and summarize output by area.

    One potential challenge is that for a large number of datasets, the datasize
    could get quite large. The Python code I have parses the XML files
    iteratively, and databases in general are designed to handle large datasets,
    so this should not be as long as the entire data can fit on the disk of a
    single machine. Beyond this point, more advanced Mongo administration or a
    different approach would become necessary. Also, Mongo performs much more
    quickly when the entire dataset can fit in memory, so once that threshold
    was passed, it could take quite a bit longer to get results.
\end{document}
